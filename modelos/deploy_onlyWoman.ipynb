{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eda6b5e4",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78cae217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f3e8a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b884c3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.machinery import SourceFileLoader\n",
    "mi_clasificador = SourceFileLoader('clasificador_padre', '../libs/clasificador_padre.py').load_module()\n",
    "model_path = \"./save_old_ow/state/ss_1.pt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3586bb74",
   "metadata": {},
   "source": [
    "CARGO EL MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b7b65e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "onlyWoman_MFCC_05_N2(\n",
       "  (inPut): Sequential(\n",
       "    (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (chanelUp): Sequential(\n",
       "    (0): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(4, 16, kernel_size=(1, 7), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(16, 32, kernel_size=(1, 7), stride=(1, 1))\n",
       "    (5): ReLU()\n",
       "    (6): Conv2d(32, 64, kernel_size=(1, 7), stride=(1, 2))\n",
       "    (7): ReLU()\n",
       "    (8): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (justTime): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(3, 7), stride=(1, 2))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 7), stride=(1, 2))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (out): Sequential(\n",
       "    (0): Linear(in_features=19712, out_features=1500, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=1500, out_features=1000, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=1000, out_features=500, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=500, out_features=100, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=100, out_features=10, bias=True)\n",
       "    (9): ReLU()\n",
       "    (10): Linear(in_features=10, out_features=3, bias=True)\n",
       "    (11): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clasificador = mi_clasificador.onlyWoman_MFCC_05_N2(['Ariana Grande', 'Katy Perry', 'Taylor Swift'])\n",
    "clasificador.load_state_dict(torch.load(model_path))\n",
    "clasificador.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e12e92",
   "metadata": {},
   "source": [
    "#PARAMETROS PARA PROCESAR EL AUDIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25b5698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parametros para los calculos del mell\n",
    "TARGET_SR = 16000  # Normalmente el audio se sule usar a 16k aunque encontre papers que trabajan a 22050 o a 22k (ver V2)\n",
    "N_FFT = 512  #muestras de la fft\n",
    "W_LEN = 400  # Numero de muestras para la ventan de la  fft (seg_de_ventan *sr) \n",
    "H_LEN = 160 # paso de la ventana entre una fft y la siguiente (paso * sr)\n",
    "N_MELS = 26\n",
    "N_MFCC = 13\n",
    "\n",
    "CHANNELS = 1\n",
    "DUR_SAMPLE = 10 # 10 segundos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d15cd8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "MFCCCalculator = torchaudio.transforms.MFCC(sample_rate = TARGET_SR,\n",
    "                                            n_mfcc = int(N_MFCC),\n",
    "                                            dct_type = 2,\n",
    "                                            norm = 'ortho',\n",
    "                                            log_mels = False,\n",
    "                                            melkwargs = {\"n_fft\": N_FFT, \"hop_length\": H_LEN, \"n_mels\": N_MELS, \"center\": False},)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c27514a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {0:\"Ari\", 1:\"Katy\", 2:\"Taylor\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c49af221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ari\n"
     ]
    }
   ],
   "source": [
    "record = sd.rec(int(DUR_SAMPLE*TARGET_SR), samplerate=TARGET_SR, channels=CHANNELS, dtype='float32', blocking= True)\n",
    "record_tensor = torch.from_numpy(record.T)\n",
    "mfcc = MFCCCalculator(record_tensor)\n",
    "res = clasificador(mfcc)\n",
    "pred = dic[int(torch.argmax(res))]\n",
    "print(pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d14476d",
   "metadata": {},
   "source": [
    "# TOMA DE MUESTRA Y PROCESADO DEL AUDIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1c677c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 160000])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b035631c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13, 997])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfcc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab878e4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
