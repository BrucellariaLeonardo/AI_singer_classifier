{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eda6b5e4",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "78cae217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "5f3e8a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "d84498de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ss_00.pt',\n",
       " 'ss_01.pt',\n",
       " 'ss_02.pt',\n",
       " 'ss_03.pt',\n",
       " 'ss_04.pt',\n",
       " 'ss_05.pt',\n",
       " 'ss_06.pt',\n",
       " 'ss_07.pt',\n",
       " 'ss_08.pt',\n",
       " 'ss_09.pt']"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('./save/state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "b884c3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.machinery import SourceFileLoader\n",
    "mi_clasificador = SourceFileLoader('clasificador_padre', '../libs/clasificador_padreV2.py').load_module()\n",
    "folder_path = \"./save/state/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "dc66ebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "version = os.listdir(folder_path)[-1]\n",
    "model_path = os.path.join(folder_path, version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "8d425ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3586bb74",
   "metadata": {},
   "source": [
    "CARGO EL MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "46d5bdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metaparameters for MFCC transformer\n",
    "#parametros para los calculos del mell\n",
    "TARGET_SR = 16000  # Normalmente el audio se sule usar a 16k aunque encontre papers que trabajan a 22050 o a 22k (ver V2)\n",
    "N_FFT = 1024  #muestras de la fft\n",
    "W_LEN = 800  # Numero de muestras para la ventan de la  fft (seg_de_ventan *sr) \n",
    "H_LEN = 320 # paso de la ventana entre una fft y la siguiente (paso * sr)\n",
    "N_MELS = 40#26\n",
    "N_MFCC = 32#13\n",
    "MFCCCalculator = torchaudio.transforms.MFCC(sample_rate = TARGET_SR,\n",
    "                                            n_mfcc = int(N_MFCC),\n",
    "                                            dct_type = 2,\n",
    "                                            norm = 'ortho',\n",
    "                                            log_mels = False,\n",
    "                                            melkwargs = \n",
    "                                            {\n",
    "                                                \"n_fft\": N_FFT,          # Size of FFT (2048)\n",
    "                                                \"win_length\": W_LEN,     # Actual window size (400 samples = 25ms)\n",
    "                                                \"hop_length\": H_LEN,     # Hop length (160 samples = 10ms)\n",
    "                                                \"n_mels\": N_MELS,        # Number of Mel bins (40)\n",
    "                                                \"center\": False\n",
    "                                                },)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "b02296f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_weight_norms(model, epoch):\n",
    "    total_l2_norm_sq = 0.0\n",
    "    print(f\"\\nEpoch {epoch+1} - Weight L2 Norms:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad and \"weight\" in name: # Comúnmente los pesos se llaman 'weight'\n",
    "            norm = torch.norm(param.data, p=2) # Calcula la norma L2\n",
    "            #print(f\"  Layer '{name}': {norm.item():.4f}\")\n",
    "            total_l2_norm_sq += norm.item()**2\n",
    "        # Podrías añadir un chequeo para 'bias' si también te interesan\n",
    "\n",
    "    overall_norm = total_l2_norm_sq**0.5\n",
    "    print(f\"  Overall Network L2 Norm (Weights Only): {overall_norm:.4f}\")\n",
    "    # Puedes guardar este valor en una lista para graficarlo después\n",
    "    return overall_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "3b7b65e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "onlyWoman_MFCC_16k_v8(\n",
       "  (transformer): MFCC(\n",
       "    (amplitude_to_DB): AmplitudeToDB()\n",
       "    (MelSpectrogram): MelSpectrogram(\n",
       "      (spectrogram): Spectrogram()\n",
       "      (mel_scale): MelScale()\n",
       "    )\n",
       "  )\n",
       "  (inPut): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (chanelUp): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (justTime): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout2d(p=0.2, inplace=False)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (out): Sequential(\n",
       "    (0): Linear(in_features=960, out_features=512, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "    (6): Linear(in_features=128, out_features=3, bias=True)\n",
       "    (7): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clasificador = mi_clasificador.onlyWoman_MFCC_16k_v8(['Ariana Grande', 'Katy Perry', 'Taylor Swift'], MFCCCalculator).to(device)\n",
    "clasificador.load_state_dict(torch.load(model_path))\n",
    "clasificador.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e12e92",
   "metadata": {},
   "source": [
    "#PARAMETROS PARA PROCESAR EL AUDIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "4c27514a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {0:\"Ari\", 1:\"Katy\", 2:\"Taylor\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "c49af221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ari\n",
      "tensor([[0.8294, 0.0686, 0.1020]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "record = sd.rec(int(DUR_SAMPLE*TARGET_SR), samplerate=TARGET_SR, channels=CHANNELS, dtype='float32', blocking= True)\n",
    "record_tensor = torch.from_numpy(record.T).to(device)\n",
    "#record_tensor = record_tensor / record_tensor.abs().max()\n",
    "record_tensor= clasificador.audio_norm(record_tensor)\n",
    "#mfcc = MFCCCalculator(record_tensor)\n",
    "clasificador.eval()\n",
    "res = clasificador(record_tensor)\n",
    "pred = dic[int(torch.argmax(res))]\n",
    "print(pred)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "0af32384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8294, 0.0686, 0.1020]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d14476d",
   "metadata": {},
   "source": [
    "# eval de pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e1c677c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 52.2903\n",
      "\n",
      "Epoch 2 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 49.1771\n",
      "\n",
      "Epoch 3 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 47.9840\n",
      "\n",
      "Epoch 4 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 47.1420\n",
      "\n",
      "Epoch 5 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 46.4043\n",
      "\n",
      "Epoch 6 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 45.7046\n",
      "\n",
      "Epoch 7 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 45.0288\n",
      "\n",
      "Epoch 8 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 44.3641\n",
      "\n",
      "Epoch 9 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 43.7156\n",
      "\n",
      "Epoch 10 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 43.0746\n",
      "\n",
      "Epoch 11 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 42.4398\n",
      "\n",
      "Epoch 12 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 41.8093\n",
      "\n",
      "Epoch 13 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 41.1916\n",
      "\n",
      "Epoch 14 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 40.5744\n",
      "\n",
      "Epoch 15 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 39.9758\n",
      "\n",
      "Epoch 16 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 39.3012\n",
      "\n",
      "Epoch 17 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 38.6430\n",
      "\n",
      "Epoch 18 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 37.9848\n",
      "\n",
      "Epoch 19 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 37.3507\n",
      "\n",
      "Epoch 20 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 36.7047\n",
      "\n",
      "Epoch 21 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 36.0960\n",
      "\n",
      "Epoch 22 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 35.5048\n",
      "\n",
      "Epoch 23 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 34.8831\n",
      "\n",
      "Epoch 24 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 34.2571\n",
      "\n",
      "Epoch 25 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 33.6844\n",
      "\n",
      "Epoch 26 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 33.0294\n",
      "\n",
      "Epoch 27 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 32.3675\n",
      "\n",
      "Epoch 28 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 31.7328\n",
      "\n",
      "Epoch 29 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 31.1510\n",
      "\n",
      "Epoch 30 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 30.5122\n",
      "\n",
      "Epoch 31 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 29.9537\n",
      "\n",
      "Epoch 32 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 29.3706\n",
      "\n",
      "Epoch 33 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 28.7307\n",
      "\n",
      "Epoch 34 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 28.1209\n",
      "\n",
      "Epoch 35 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 27.5747\n",
      "\n",
      "Epoch 36 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 27.0062\n",
      "\n",
      "Epoch 37 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 26.4397\n",
      "\n",
      "Epoch 38 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 25.8720\n",
      "\n",
      "Epoch 39 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 25.2905\n",
      "\n",
      "Epoch 40 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 24.7044\n",
      "\n",
      "Epoch 41 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 24.0960\n",
      "\n",
      "Epoch 42 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 23.4797\n",
      "\n",
      "Epoch 43 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 22.8916\n",
      "\n",
      "Epoch 44 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 22.2720\n",
      "\n",
      "Epoch 45 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 21.6695\n",
      "\n",
      "Epoch 46 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 21.1038\n",
      "\n",
      "Epoch 47 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 20.5341\n",
      "\n",
      "Epoch 48 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 19.9821\n",
      "\n",
      "Epoch 49 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 19.4435\n",
      "\n",
      "Epoch 50 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 18.9373\n",
      "\n",
      "Epoch 51 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 18.4222\n",
      "\n",
      "Epoch 52 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 17.9359\n",
      "\n",
      "Epoch 53 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 17.4234\n",
      "\n",
      "Epoch 54 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 16.9534\n",
      "\n",
      "Epoch 55 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 16.5046\n",
      "\n",
      "Epoch 56 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 16.0232\n",
      "\n",
      "Epoch 57 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 15.5360\n",
      "\n",
      "Epoch 58 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 15.1439\n",
      "\n",
      "Epoch 59 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 14.6791\n",
      "\n",
      "Epoch 60 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 14.2868\n",
      "\n",
      "Epoch 61 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 13.8323\n",
      "\n",
      "Epoch 62 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 13.4322\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(os.listdir(folder_path))):\n",
    "    patito = os.path.join(folder_path, os.listdir(folder_path)[i])\n",
    "    clasificador.load_state_dict(torch.load(patito))\n",
    "    log_weight_norms(clasificador, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab878e4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
