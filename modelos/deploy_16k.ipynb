{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eda6b5e4",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "78cae217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5f3e8a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d84498de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ss_30.pt'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('./save/state')[-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b884c3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.machinery import SourceFileLoader\n",
    "mi_clasificador = SourceFileLoader('clasificador_padre', '../libs/clasificador_padreV2.py').load_module()\n",
    "folder_path = \"./save/state/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dc66ebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "version = os.listdir(folder_path)[-1]\n",
    "model_path = os.path.join(folder_path, version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8d425ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3586bb74",
   "metadata": {},
   "source": [
    "CARGO EL MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "46d5bdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metaparameters for MFCC transformer\n",
    "#parametros para los calculos del mell\n",
    "TARGET_SR = 16000  # Normalmente el audio se sule usar a 16k aunque encontre papers que trabajan a 22050 o a 22k (ver V2)\n",
    "N_FFT = 1024  #muestras de la fft\n",
    "W_LEN = 800  # Numero de muestras para la ventan de la  fft (seg_de_ventan *sr) \n",
    "H_LEN = 320 # paso de la ventana entre una fft y la siguiente (paso * sr)\n",
    "N_MELS = 40#26\n",
    "N_MFCC = 32#13\n",
    "\n",
    "MFCCCalculator = torchaudio.transforms.MFCC(sample_rate = TARGET_SR,\n",
    "                                            n_mfcc = int(N_MFCC),\n",
    "                                            dct_type = 2,\n",
    "                                            norm = 'ortho',\n",
    "                                            log_mels = False,\n",
    "                                            melkwargs = \n",
    "                                            {\n",
    "                                                \"n_fft\": N_FFT,          # Size of FFT (2048)\n",
    "                                                \"win_length\": W_LEN,     # Actual window size (400 samples = 25ms)\n",
    "                                                \"hop_length\": H_LEN,     # Hop length (160 samples = 10ms)\n",
    "                                                \"n_mels\": N_MELS,        # Number of Mel bins (40)\n",
    "                                                \"center\": False\n",
    "                                                },)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b02296f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_weight_norms(model, epoch):\n",
    "    total_l2_norm_sq = 0.0\n",
    "    print(f\"\\nEpoch {epoch+1} - Weight L2 Norms:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad and \"weight\" in name: # Comúnmente los pesos se llaman 'weight'\n",
    "            norm = torch.norm(param.data, p=2) # Calcula la norma L2\n",
    "            #print(f\"  Layer '{name}': {norm.item():.4f}\")\n",
    "            total_l2_norm_sq += norm.item()**2\n",
    "        # Podrías añadir un chequeo para 'bias' si también te interesan\n",
    "\n",
    "    overall_norm = total_l2_norm_sq**0.5\n",
    "    print(f\"  Overall Network L2 Norm (Weights Only): {overall_norm:.4f}\")\n",
    "    # Puedes guardar este valor en una lista para graficarlo después\n",
    "    return overall_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2ecad6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#{'Ariana Grande': tensor([1., 0., 0., 0., 0.]), 'Katy Perry': tensor([0., 1., 0., 0., 0.]), 'Metallica': tensor([0., 0., 1., 0., 0.]), 'Pink Floyd': tensor([0., 0., 0., 1., 0.]), 'Taylor Swift': tensor([0., 0., 0., 0., 1.])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3b7b65e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "onlyWoman_MFCC_16k_v8(\n",
       "  (transformer): MFCC(\n",
       "    (amplitude_to_DB): AmplitudeToDB()\n",
       "    (MelSpectrogram): MelSpectrogram(\n",
       "      (spectrogram): Spectrogram()\n",
       "      (mel_scale): MelScale()\n",
       "    )\n",
       "  )\n",
       "  (inPut): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (chanelUp): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (justTime): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout2d(p=0.2, inplace=False)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (out): Sequential(\n",
       "    (0): Linear(in_features=960, out_features=512, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "    (6): Linear(in_features=128, out_features=5, bias=True)\n",
       "    (7): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clasificador = mi_clasificador.onlyWoman_MFCC_16k_v8(['Ariana Grande', 'Katy Perry', 'Metallica', 'Pink Floyd', 'Taylor Swift'], MFCCCalculator).to(device)\n",
    "clasificador.load_state_dict(torch.load(model_path))\n",
    "clasificador.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e12e92",
   "metadata": {},
   "source": [
    "#PARAMETROS PARA PROCESAR EL AUDIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4c27514a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {0:'Ariana Grande', 1:'Katy Perry', 2:'Metallica', 3:'Pink Floyd', 4:'Taylor Swift'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c49af221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ariana Grande\n",
      "tensor([[0.5105, 0.4409, 0.0153, 0.0161, 0.0171]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "record = sd.rec(int(10*TARGET_SR), samplerate=TARGET_SR, channels=1, dtype='float32', blocking= True)\n",
    "record_tensor = torch.from_numpy(record.T).to(device)\n",
    "#record_tensor = record_tensor / record_tensor.abs().max()\n",
    "record_tensor= clasificador.audio_norm(record_tensor)\n",
    "#mfcc = MFCCCalculator(record_tensor)\n",
    "clasificador.eval()\n",
    "res = clasificador(record_tensor)\n",
    "pred = dic[int(torch.argmax(res))]\n",
    "print(pred)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0af32384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0367, 0.9397, 0.0075, 0.0079, 0.0083]], device='cuda:0',\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d14476d",
   "metadata": {},
   "source": [
    "# eval de pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e1c677c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 15.3236\n",
      "\n",
      "Epoch 2 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 13.7884\n",
      "\n",
      "Epoch 3 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 12.9787\n",
      "\n",
      "Epoch 4 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 12.3871\n",
      "\n",
      "Epoch 5 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 11.8943\n",
      "\n",
      "Epoch 6 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 11.4483\n",
      "\n",
      "Epoch 7 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 11.0435\n",
      "\n",
      "Epoch 8 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 10.6748\n",
      "\n",
      "Epoch 9 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 10.3452\n",
      "\n",
      "Epoch 10 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 10.0488\n",
      "\n",
      "Epoch 11 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 9.7898\n",
      "\n",
      "Epoch 12 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 9.5693\n",
      "\n",
      "Epoch 13 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 9.3854\n",
      "\n",
      "Epoch 14 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 9.2361\n",
      "\n",
      "Epoch 15 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 9.1256\n",
      "\n",
      "Epoch 16 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 9.0355\n",
      "\n",
      "Epoch 17 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 8.9667\n",
      "\n",
      "Epoch 18 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 8.9083\n",
      "\n",
      "Epoch 19 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 8.8605\n",
      "\n",
      "Epoch 20 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 8.8182\n",
      "\n",
      "Epoch 21 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 8.7815\n",
      "\n",
      "Epoch 22 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 8.7407\n",
      "\n",
      "Epoch 23 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 8.7234\n",
      "\n",
      "Epoch 24 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 8.6874\n",
      "\n",
      "Epoch 25 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 8.6555\n",
      "\n",
      "Epoch 26 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 8.6319\n",
      "\n",
      "Epoch 27 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 8.6043\n",
      "\n",
      "Epoch 28 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 8.5847\n",
      "\n",
      "Epoch 29 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 8.5592\n",
      "\n",
      "Epoch 30 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 8.5321\n",
      "\n",
      "Epoch 31 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 8.5133\n",
      "\n",
      "Epoch 32 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 8.4902\n",
      "\n",
      "Epoch 33 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 8.4722\n",
      "\n",
      "Epoch 34 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 8.4528\n",
      "\n",
      "Epoch 35 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 8.4324\n",
      "\n",
      "Epoch 36 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 8.4149\n",
      "\n",
      "Epoch 37 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 8.3968\n",
      "\n",
      "Epoch 38 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 8.3815\n",
      "\n",
      "Epoch 39 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 8.3605\n",
      "\n",
      "Epoch 40 - Weight L2 Norms:\n",
      "  Overall Network L2 Norm (Weights Only): 8.3452\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(os.listdir(folder_path))):\n",
    "    patito = os.path.join(folder_path, os.listdir(folder_path)[i])\n",
    "    clasificador.load_state_dict(torch.load(patito))\n",
    "    log_weight_norms(clasificador, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab878e4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
