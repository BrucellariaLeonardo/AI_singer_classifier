# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kKjQo3SPzPgEL81LK1XqmPBOxK-Bv-Gd
"""


"""
El sistema esta pensado para que definas bloques Data_in, h0, h1, h2, h3... hn, Data_out.
De modo que en hardcode se definen que capas usar, pero la cantidad de neuronas de cada capa,
quedan libres para modificarlas cuando se llama constructor de la clase medinte el vector architecture. 
Despues, clases es un vector, donde los indices mapean a la label de lo que se quiere clasificar.
"""

import torch
from torch import nn
from tqdm import tqdm
import torchaudio

class _Clasificador_padre(nn.Module):
  def __init__(self, classes):
    """Hijo, necesito que generes una arquitectura"""
    super().__init__()
    self.classes = classes.copy()
    
  def forward(self):
    """Implementado por hijo"""
    pass
  
  def apply_rir(self, input_batch, rir_batch):
    """
    Convolucion el input_batch con el rir_batch, obteniendo el mismo largo temporal que input_batch
    """
    return torchaudio.functional.fftconvolve(input_batch, rir_batch, "full")[:, :input_batch.shape[1]]

  def audio_norm(self, input_batch):
    """ Recive bache de audio y lo normaliza entre -1 y 1. 
        Si hay valores muy cercanos a cero, los levanta a 1e-12
    """
    max_abs_val = input_batch.abs().max(dim = 1, keepdim = True)[0]
    return input_batch / max_abs_val #TODO Agregar el threshold

  def train_epoch(self, data_loader, optimizer, loss_fn, acuracy_fn, device):
    """
    + El data_loader debe retornar primero el input, segundo el target y tercera la seniar rir
    + El acuracy_fn debe retornar un tensor
    + Acurracy_fn debe poder recibir una prediccion en formato Tensor y un target en formato Tensor
    """
    loss_acum = 0
    accuracy_acum = 0
    for input_batch, target_batch, rir_batch in data_loader:
      #datos a la GPU o CPU
      input_batch = input_batch.to(device)
      target_batch = target_batch.to(device)
      rir_batch = rir_batch.to(device)
      #convoluciono la entrada con el rir
      input_batch = self.apply_rir(input_batch, rir_batch)
      #normalizo la senial 
      input_batch = self.audio_norm(input_batch)
      #prediccion y optimizacion
      optimizer.zero_grad()
      prediccion = self(input_batch)
      loss = loss_fn(prediccion, target_batch)
      loss.backward()
      optimizer.step()
      #metricas
      with torch.no_grad():
        loss_acum += loss.item()
        accuracy_acum += acuracy_fn(prediccion, target_batch)
      #Calculo de la media de las metricas
    with torch.no_grad():
      accuracy_acum /= len(data_loader)
      loss_acum /= len(data_loader)
    return loss_acum, accuracy_acum  

  def train_loop(self, epochs, data_loader, optimizer, loss_fn, acuracy_fn, device):
    """
    + El data_loader debe retornar primero el input y segundo el target
    + El acuracy_fn debe retornar un tensor
    + Acurracy_fn debe poder recibir una prediccion en formato Tensor y un target en formato Tensor
    """
    self.train()
    loss_list = torch.empty(0)
    accuracy_list = torch.empty(0)
    for epoch in range(epochs):
      new_loss, new_accuracy = self.train_epoch(data_loader, optimizer, loss_fn, acuracy_fn, device)
      #Guardo la metrica
      with torch.no_grad():
        loss_list.resize_(epoch + 1) #agrego un espacio para el loss que acabo de calcular
        loss_list[- 1] = new_loss #inserto el loss nuevo en el lugar que le cree en la linea de arriba
        accuracy_list.resize_(epoch + 1)
        accuracy_list[- 1] = new_accuracy
        print(f"Epoch {epoch} of {epochs} / Loss: {loss_list[-1]} / Accuracy: {accuracy_list[-1]}")
    return loss_list, accuracy_list

  def evaluate(self, data_loader, loss_fn, acuracy_fn, device):
    """
    Para evaluar decido no aplicar los rir
    """
    self.eval()
    with torch.no_grad():
      loss_acum = 0
      accuracy_acum = 0
      for input_batch, target_batch, rir_batch in data_loader:
        #datos a GPU o CPU
        input_batch = input_batch.to(device)
        target_batch = target_batch.to(device)
        rir_batch = rir_batch.to(device)
        #convoluciono la entrada con el rir
        input_batch = self.apply_rir(input_batch, rir_batch)
        #normalizo la senial
        input_batch = self.audio_norm(input_batch)
        #prediccion
        prediccion = self(input_batch)
        loss = loss_fn(prediccion, target_batch)
        #metricas
        loss_acum += loss.item()
        accuracy_acum += acuracy_fn(prediccion, target_batch)
      #Calculo de la media de las metricas
      accuracy_acum /= len(data_loader)
      loss_acum /= len(data_loader)
      return loss_acum, accuracy_acum

  def classes(self):
    """
    Devuelve una lista con las clases del modelo
    """
    return self.classes.copy()

  def save_model(self, fname):
    """
    Esta funcion guarda un modelo que se guardo con un formato específico, el cual corresponde a el state_dict de una red neuronal al que se le agrega un
    campo que indica la arquitectura de la red "architecture" y un campo que indica las clases del modelo "classes"

    Args:
      modelo: Modelo a guardar
      name: Nombre del archivo destino

    Returns:
      saveState: diccionario guardado en el archivo
    """
    saveState = self.state_dict()
    saveState['classes'] = self.classes()
    torch.save(saveState, f"{self.name}_{fname}")
    return None

def load_model_file(l_file):
  """
  Esta funcion carga un modelo que se guardo con un formato específico, el cual corresponde a el state_dict de una red neuronal al que se le agrega un
  campo que indica la arquitectura de la red "architecture" y un campo que indica las clases del modelo "classes"

  Args:
    l_file: Nombre del archivo destino

  Returns:
    Clases del modelo cargado
    Parametros del modelo cargado
  """
  saveState = torch.load(l_file)
  classes = saveState.pop('classes')
  return classes, saveState


class onlyWoman_MFCC_05_N2(_Clasificador_padre):
  def __init__(self, classes):
    super().__init__(classes)
    self.name = "clas_02"
    #Variables de entorno
    ###
    #Baloques de la red
    self.inPut = nn.Sequential( 
        nn.Conv2d(1,4,3,1,2),
        nn.ReLU() )

    self.chanelUp =  nn.Sequential(
      nn.BatchNorm2d(4),
      nn.ReLU(),


      nn.Conv2d(4,16,(1,7),1,0),
      nn.ReLU(),


      nn.Conv2d(16,32,(1,7),1,0),
      nn.ReLU(),
      
      #nn.Dropout(0.5),
      nn.Conv2d(32,64,(1,7),(1,2),0),
      nn.ReLU(),
      
      nn.MaxPool2d(kernel_size= (1,2)),
      nn.BatchNorm2d(64)
      )

    self.justTime =  nn.Sequential( #bloque que recibe los datos y los acondiciona
      #nn.Dropout(0.5),
      nn.Conv2d(64,64,(3,7),(1,2),0),
      nn.ReLU(),

      nn.Conv2d(64,64,(3,7),(1,2),0),
      nn.ReLU(),

      nn.MaxPool2d(kernel_size= (1,2)),
      nn.BatchNorm2d(64) )
 
    self.out = nn.Sequential(   # bloque que acondiciona la salida
        nn.Linear(19712,1500), #Lineal que sale a las clases objetivo
        nn.ReLU(),
        nn.Linear(1500,1000), #Lineal que sale a las clases objetivo
        nn.ReLU(),
        nn.Linear(1000,500), #Lineal que sale a las clases objetivo
        nn.ReLU(),
        nn.Linear(500,100), #Lineal que sale a las clases objetivo
        nn.ReLU(),
        nn.Linear(100,10), #Lineal que sale a las clases objetivo
        nn.ReLU(),
        nn.Linear(10,3), #Lineal que sale a las clases objetivo
        nn.Softmax( dim = 1))


  def forward(self, x):
    #print("In:", x.shape)            
    x = x.unsqueeze(1)
    #print("squeeze:", x.shape)
    prediccion = self.inPut(x)
    prediccion = self.chanelUp(prediccion)
    prediccion = self.justTime(prediccion)
    #  print(prediccion.shape)         ######################################################################
    prediccion = torch.flatten(prediccion,1)
    #print("post flatten:", prediccion.shape)
    prediccion = self.out(prediccion)
    return  prediccion

class onlyWoman_MFCC_16k(onlyWoman_MFCC_05_N2):
  def __init__(self, classes, MFCC_transforemr = None):
    super().__init__(classes)
    self.transformer = MFCC_transforemr
    self.name = "MFCC_16k"


  def forward(self, x):
    #print("In:", x.shape)            
    if(self.transformer != None):
      x = self.transformer(x)
    #print("Transformed:", x.shape)
    prediccion = super().forward(x)
    #x = x.unsqueeze(1)
    #print("squeeze:", x.shape)
    #prediccion = self.inPut(x)
    #prediccion = self.chanelUp(prediccion)
    #prediccion = self.justTime(prediccion)
    #  print(prediccion.shape)         ######################################################################
    #prediccion = torch.flatten(prediccion,1)
    #print("post flatten:", prediccion.shape)
    #prediccion = self.out(prediccion)
    #print("prediccion:", prediccion.shape)
    return  prediccion

class onlyWoman_MFCC_16k_v2(onlyWoman_MFCC_05_N2):
  def __init__(self, classes, MFCC_transforemr = None):
    super().__init__(classes)
    self.transformer = MFCC_transforemr
    self.name = "MFCC_16k"


  def forward(self, x):
    #print("In:", x.shape)            
    if(self.transformer != None):
      x = self.transformer(x)
    #print("Transformed:", x.shape)
    prediccion = super().forward(x)
    #x = x.unsqueeze(1)
    #print("squeeze:", x.shape)
    #prediccion = self.inPut(x)
    #prediccion = self.chanelUp(prediccion)
    #prediccion = self.justTime(prediccion)
    #  print(prediccion.shape)         ######################################################################
    #prediccion = torch.flatten(prediccion,1)
    #print("post flatten:", prediccion.shape)
    #prediccion = self.out(prediccion)
    #print("prediccion:", prediccion.shape)
    return  prediccion

  def _train_epoch(self, data_loader, optimizer, loss_fn, acuracy_fn, device):
    """
    + El data_loader debe retornar primero el input y segundo el target
    + El acuracy_fn debe retornar un tensor
    + Acurracy_fn debe poder recibir una prediccion en formato Tensor y un target en formato Tensor
    """
    loss_acum = 0
    accuracy_acum = 0
    b = 1
    for input_batch, target_batch, rir_batch in data_loader:
      #datos a la GPU o CPU (segun lo que me pasaron)
      input_batch = input_batch.to(device)
      target_batch = target_batch.to(device)
      rir_batch = rir_batch.to(device)
      #convoluciono la entrada con el rir
      input_batch = torchaudio.functional.fftconvolve(input_batch, rir_batch, "full") #[:input_batch.shape[1]]
      input_batch = input_batch[:,:160000]
        #print("Input_batch:",input_batch.shape)
      #normalizo la senial del rir
      max_abs_val = input_batch.abs().max(dim = 1, keepdim = True)[0]
        #print("max_abs: ", max_abs_val.shape)
      input_batch = input_batch / max_abs_val
        #print("normalized input_batch:", input_batch.shape)
      #prediccion y optimizacion
      optimizer.zero_grad()
      prediccion = self(input_batch)
      loss = loss_fn(prediccion, target_batch)
      loss.backward()
      optimizer.step()
      #metricas
      with torch.no_grad():
        loss_acum += loss.item()
        accuracy_acum += acuracy_fn(prediccion, target_batch)
        #if b % 10 == 0:
        #  print(f"Data: {b*len(input_batch)}/ {len(data_loader*len(input_batch))}")
      #Calculo de la media de las metricas
    with torch.no_grad():
      accuracy_acum /= len(data_loader)
      loss_acum /= len(data_loader)
    return loss_acum, accuracy_acum  

  def evaluate(self, data_loader, loss_fn, acuracy_fn, device):
    """
    """
    self.eval()
    with torch.no_grad():
      loss_acum = 0
      accuracy_acum = 0
      for input_batch, target_batch, _ in tqdm(data_loader):
        input_batch = input_batch.to(device)
        target_batch = target_batch.to(device)
        
        #prediccion y optimizacion
        prediccion = self(input_batch)
        loss = loss_fn(prediccion, target_batch)
        #metricas
        loss_acum += loss.item()
        accuracy_acum += acuracy_fn(prediccion, target_batch)
      #Calculo de la media de las metricas
      accuracy_acum /= len(data_loader)
      loss_acum /= len(data_loader)
      return loss_acum, accuracy_acum

class onlyWoman_MFCC_T(_Clasificador_padre):
  def __init__(self, classes):
    super().__init__(classes)
    self.name = "clas_02"
    #Variables de entorno
    ###
    #Baloques de la red
    self.inPut = nn.Sequential( 
        nn.Conv2d(1,4,3,1,(1,2)),
        nn.ReLU() )

    self.chanelUp =  nn.Sequential(
      #nn.Dropout(0.5),
      nn.Conv2d(4,4,(1,3),1,0),
      nn.ReLU(),
      nn.Conv2d(4,16,(1,7),1,0),
      nn.ReLU(),

      #nn.Conv2d(16,16,(1,5),1,0),
      #nn.ReLU(),
      nn.Conv2d(16,32,(1,7),1,0),
      nn.ReLU(),
      
      #nn.Conv2d(32,32,(1,7),1,0),
      #nn.ReLU(),
      nn.Conv2d(32,64,(1,7),1,0),
      nn.ReLU(),
      
      nn.MaxPool2d(kernel_size= (1,2)) 
      )

    self.extract =  nn.Sequential( #bloque que recibe los datos y los acondiciona
      #nn.Dropout(0.5),
      nn.Conv2d(64,64,(3,7),(1,2),0),
      nn.ReLU(),
      #nn.Conv2d(64,64,(3,7),(1,2),0),
      #nn.ReLU(),
      #nn.Conv2d(64,64,(3,7),(1,2),0),
      #nn.ReLU(),
      nn.Conv2d(64,32,(3,7),(1,2),0),
      nn.ReLU(),
      nn.Conv2d(32,16,(3,7),(1,2),0),
      nn.ReLU())
      #nn.MaxPool2d(kernel_size= (2,2)) )
 
    self.out = nn.Sequential(   # bloque que acondiciona la salida
        nn.Linear(6272,512), #Lineal que sale a las clases objetivo
        nn.ReLU(),
        nn.Linear(512,32), #Lineal que sale a las clases objetivo
        nn.ReLU(),
        nn.Linear(32,16), #Lineal que sale a las clases objetivo
        nn.ReLU(),
        nn.Linear(16,3), #Lineal que sale a las clases objetivo
        nn.Softmax( dim = 1))


  def forward(self, x, v = False):
    if v:
      #print("In:", x.shape)            
      x = x.unsqueeze(1)
      print("squeeze:", x.shape)
      prediccion = self.inPut(x)
      print("post 1x1:", x.shape)
      prediccion = self.chanelUp(prediccion)
      print("post chanelUp:", prediccion.shape)
      prediccion = self.extract(prediccion)
      print("post extract:", prediccion.shape)
      prediccion = torch.flatten(prediccion,1)
      print("post flatten:", prediccion.shape)
      prediccion = self.out(prediccion)
    else:
      x = x.unsqueeze(1)
      prediccion = self.inPut(x)
      prediccion = self.chanelUp(prediccion)
      prediccion = self.extract(prediccion)
      prediccion = torch.flatten(prediccion,1)
      prediccion = self.out(prediccion)
    return  prediccion


##########################################################
#####  Variacion de chanel up ##################
#########################################

class onlyWoman_MFCC_16k_v3(_Clasificador_padre):
  def __init__(self, classes, MFCC_transformer = None):
    super().__init__(classes)
    self.transformer = MFCC_transformer
    self.name = "MFCC_16k"
    ########################
    ## Baloques de la red ##
    ########################
    self.inPut = nn.Sequential( 
        nn.Conv2d(1,4,3,1,2),
        nn.ReLU() )

    self.chanelUp =  nn.Sequential(
      nn.BatchNorm2d(4),
      nn.ReLU(),


      nn.Conv2d(4,8,(1,3),1,0),
      nn.ReLU(),


      nn.Conv2d(8,16,(1,5),1,0),
      nn.ReLU(),
      
      #nn.Dropout(0.5),
      nn.Conv2d(16,32,(1,7),(1,2),0),
      nn.ReLU(),

      nn.Conv2d(32,64,(1,7),(1,2),0),
      nn.ReLU(),
      
      nn.MaxPool2d(kernel_size= (1,2)),
      nn.BatchNorm2d(64)
      )

    self.justTime =  nn.Sequential( #bloque que recibe los datos y los acondiciona
      #nn.Dropout(0.5),
      nn.Conv2d(64,64,(3,7),(1,2),0),
      nn.ReLU(),

      nn.Conv2d(64,64,(3,7),(1,2),0),
      nn.ReLU(),
      
      nn.Conv2d(64,64,(5,7),(1,2),0),
      nn.ReLU(),

      nn.Conv2d(64,64,(5,7),(1,2),0),
      nn.ReLU(),

      nn.MaxPool2d(kernel_size= (1,2)),
      nn.BatchNorm2d(64) )
 
    self.out = nn.Sequential(   # bloque que acondiciona la salida
        nn.Linear(2240,1500), #Lineal que sale a las clases objetivo
        nn.ReLU(),
        nn.Linear(1500,1000), #Lineal que sale a las clases objetivo
        nn.ReLU(),
        nn.Linear(1000,500), #Lineal que sale a las clases objetivo
        nn.ReLU(),
        nn.Linear(500,100), #Lineal que sale a las clases objetivo
        nn.ReLU(),
        nn.Linear(100,10), #Lineal que sale a las clases objetivo
        nn.ReLU(),
        nn.Linear(10,3), #Lineal que sale a las clases objetivo
        nn.Softmax( dim = 1))

  def forward(self, x):
    #print("In:", x.shape)            
    if(self.transformer != None):
      x = self.transformer(x)
    #print("Transformed:", x.shape)
    #print("In:", x.shape)            
    x = x.unsqueeze(1)
    #print("squeeze:", x.shape)
    x = self.inPut(x)
    x = self.chanelUp(x)
    x = self.justTime(x)
    #  print(x.shape)         
    x = torch.flatten(x,1)
    #print("post flatten:", x.shape)
    x = self.out(x)
    return  x

##################
####### V4 #######
##################

class onlyWoman_MFCC_16k_v4(_Clasificador_padre):
  def __init__(self, classes, MFCC_transformer = None):
    super().__init__(classes)
    self.transformer = MFCC_transformer
    self.name = "MFCC_16k"
    ########################
    ## Baloques de la red ##
    ########################
    self.inPut = nn.Sequential( 
        nn.Conv2d(1,4,3,1,2),
        nn.ReLU() )

    self.chanelUp =  nn.Sequential(
      nn.BatchNorm2d(4),
      nn.ReLU(),


      nn.Conv2d(4,8,(1,3),1,0),
      nn.ReLU(),


      nn.Conv2d(8,16,(1,5),1,0),
      nn.ReLU(),
      
      #nn.Dropout(0.5),
      nn.Conv2d(16,32,(1,7),(1,2),0),
      nn.ReLU(),

      nn.Conv2d(32,64,(1,7),(1,2),0),
      nn.ReLU(),
      
      nn.Conv2d(64,128,(1,7),(1,2),0),
      nn.ReLU(),
      
      nn.MaxPool2d(kernel_size= (1,2)),
      nn.BatchNorm2d(128)
      )

    self.justTime =  nn.Sequential( #bloque que recibe los datos y los acondiciona
      #nn.Dropout(0.5),
      nn.Conv2d(128,256,(3,7),(1,2),0),
      nn.ReLU(),

      nn.Conv2d(256,512,(3,7),(1,2),0),
      nn.ReLU(),

      nn.Conv2d(512,512,(5,7),(1,2),0),
      nn.ReLU(),

      #nn.Conv2d(128,128,(5,7),(1,2),0),
      #nn.ReLU(),

      nn.MaxPool2d(kernel_size= (1,2)),
      nn.BatchNorm2d(512) )
 
    self.out = nn.Sequential(   # bloque que acondiciona la salida
        nn.Linear(3584,3584), #Lineal que sale a las clases objetivo
        nn.ReLU(),
        nn.Linear(3584,1500), #Lineal que sale a las clases objetivo
        nn.ReLU(),
        nn.Linear(1500,500), #Lineal que sale a las clases objetivo
        nn.ReLU(),
        nn.Linear(500,100), #Lineal que sale a las clases objetivo
        nn.ReLU(),
        nn.Linear(100,10), #Lineal que sale a las clases objetivo
        nn.ReLU(),
        nn.Linear(10,3), #Lineal que sale a las clases objetivo
        nn.Softmax( dim = 1))

  def forward(self, x):
    #print("In:", x.shape)            
    if(self.transformer != None):
      x = self.transformer(x)
    #print("Transformed:", x.shape)
    #print("In:", x.shape)            
    x = x.unsqueeze(1)
    #print("squeeze:", x.shape)
    x = self.inPut(x)
    x = self.chanelUp(x)
    x = self.justTime(x)
    #  print(x.shape)         
    x = torch.flatten(x,1)
    #print("post flatten:", x.shape)
    x = self.out(x)
    return  x

##################
####### V5 #######
##################

class onlyWoman_MFCC_16k_v5(_Clasificador_padre):
  def __init__(self, classes, MFCC_transformer = None):
    super().__init__(classes)
    self.transformer = MFCC_transformer
    self.name = "MFCC_16k"
    ########################
    ## Baloques de la red ##
    ########################
    self.inPut = nn.Sequential( 
        nn.Conv2d(1,4,3,1,2),
        nn.ReLU() )

    self.chanelUp =  nn.Sequential(
      nn.BatchNorm2d(4),
      nn.ReLU(),


      nn.Conv2d(4,8,(1,3),1,0),
      nn.ReLU(),


      nn.Conv2d(8,16,(1,5),1,0),
      nn.ReLU(),
      
      #nn.Dropout(0.5),
      nn.Conv2d(16,32,(1,7),(1,2),0),
      nn.ReLU(),

      nn.Conv2d(32,64,(1,7),(1,2),0),
      nn.ReLU(),
      
      nn.Conv2d(64,128,(1,7),(1,2),0),
      nn.ReLU(),
      
      nn.MaxPool2d(kernel_size= (1,2)),
      nn.BatchNorm2d(128)
      )

    self.justTime =  nn.Sequential( #bloque que recibe los datos y los acondiciona
      #nn.Dropout(0.5),
      nn.Conv2d(128,256,(3,7),(1,2),0),
      nn.ReLU(),

      nn.Conv2d(256,512,(3,7),(1,2),0),
      nn.ReLU(),

      nn.Conv2d(512,512,(5,7),(1,2),0),
      nn.ReLU(),

      #nn.Conv2d(128,128,(5,7),(1,2),0),
      #nn.ReLU(),

      nn.MaxPool2d(kernel_size= (1,2)),
      nn.BatchNorm2d(512) )
 
    self.out = nn.Sequential(   # bloque que acondiciona la salida
        nn.Linear(3584,3584), #Lineal que sale a las clases objetivo
        nn.ReLU(),
        nn.Linear(3584,3000), #Lineal que sale a las clases objetivo
        nn.ReLU(),
        nn.Linear(3000,1500), #Lineal que sale a las clases objetivo
        nn.ReLU(),
        nn.Linear(1500,1000), #Lineal que sale a las clases objetivo
        nn.ReLU(),
        nn.Linear(1000,500), #Lineal que sale a las clases objetivo
        nn.ReLU(),
        nn.Linear(500,100), #Lineal que sale a las clases objetivo
        nn.ReLU(),
        nn.Linear(100,10), #Lineal que sale a las clases objetivo
        nn.ReLU(),
        nn.Linear(10,3), #Lineal que sale a las clases objetivo
        nn.Softmax( dim = 1))


  def forward(self, x):
    #print("In:", x.shape)            
    if(self.transformer != None):
      x = self.transformer(x)
    #print("Transformed:", x.shape)
    #print("In:", x.shape)            
    x = x.unsqueeze(1)
    #print("squeeze:", x.shape)
    x = self.inPut(x)
    x = self.chanelUp(x)
    x = self.justTime(x)
    #  print(x.shape)         
    x = torch.flatten(x,1)
    #print("post flatten:", x.shape)
    x = self.out(x)
    return  x

##################
####### V6 #######
##################

class onlyWoman_MFCC_16k_v6(_Clasificador_padre):
  """Copia de la arquitectura VGGISH"""
  def __init__(self, classes, MFCC_transformer = None):
    super().__init__(classes)
    self.transformer = MFCC_transformer
    self.name = "VGGISH"
    ########################
    ## Baloques de la red ##
    ########################
    self.inPut = nn.Sequential(
      nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1,),
      nn.ReLU(inplace=True),
      nn.MaxPool2d(kernel_size=2, stride=2)
      )

    self.chanelUp =  nn.Sequential(
      nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1,),
      nn.ReLU(inplace=True),
      nn.MaxPool2d(kernel_size=2, stride=2),

      nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1,),
      nn.ReLU(inplace=True),
      nn.MaxPool2d(kernel_size=2, stride=2),

      nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1,),
      nn.ReLU(inplace=True),
      nn.MaxPool2d(kernel_size=2, stride=2)


      )

    self.justTime =  nn.Sequential( #bloque que recibe los datos y los acondiciona
      nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1,),
      nn.ReLU(inplace=True),
      nn.MaxPool2d(kernel_size=2, stride=2)
        )
 
    self.out = nn.Sequential(   # bloque que acondiciona la salida
        nn.Linear(7936,4096), #Lineal que sale a las clases objetivo
        nn.ReLU(inplace=True),
        nn.Linear(4096,128), #Lineal que sale a las clases objetivo
        nn.ReLU(inplace=True),
        nn.Linear(128,3), #Lineal que sale a las clases objetivo
        nn.Softmax( dim = 1))


  def forward(self, x):
    #print("In:", x.shape)            
    if(self.transformer != None):
      x = self.transformer(x)
    #print("Transformed:", x.shape)
    #print("In:", x.shape)            
    x = x.unsqueeze(1)
    #print("squeeze:", x.shape)
    x = self.inPut(x)
    #print("inPut_res:", x.shape)
    x = self.chanelUp(x)
    #print("chanelUp_res:", x.shape)
    x = self.justTime(x)
    #print("justTime_res:", x.shape)
    #  print(x.shape)         
    x = torch.flatten(x,1)
    #print("post flatten:", x.shape)
    x = self.out(x)
    return  x



##################
####### V6.1 #######
##################

class onlyWoman_MFCC_16k_v6_1(_Clasificador_padre):
  """Copia de la arquitectura VGGISH. Agrego capas de batchnorm"""
  def __init__(self, classes, MFCC_transformer = None):
    super().__init__(classes)
    self.transformer = MFCC_transformer
    self.name = "VGGISH"
    ########################
    ## Baloques de la red ##
    ########################
    self.inPut = nn.Sequential(
      nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1,),
      nn.BatchNorm2d(32),
      nn.ReLU(inplace=True),
      nn.Dropout2d(0.2),
      nn.MaxPool2d(kernel_size=2, stride=2)
      )

    self.chanelUp =  nn.Sequential(
      nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1,),
      nn.ReLU(inplace=True),
      nn.MaxPool2d(kernel_size=2, stride=2),

      nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1,),
      nn.ReLU(inplace=True),
      nn.MaxPool2d(kernel_size=2, stride=2),

      nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1,),
      nn.BatchNorm2d(256),
      nn.ReLU(inplace=True),
      nn.Dropout2d(0.2),
      nn.MaxPool2d(kernel_size=2, stride=2)


      )

    self.justTime =  nn.Sequential( #bloque que recibe los datos y los acondiciona
      nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1,),
      nn.ReLU(inplace=True),
      nn.Dropout2d(0.2),
      nn.MaxPool2d(kernel_size=2, stride=2)
        )
 
    self.out = nn.Sequential(   # bloque que acondiciona la salida
        nn.Linear(7936,4096), #Lineal que sale a las clases objetivo
        nn.ReLU(inplace=True),
        nn.Dropout(0.2),
        nn.Linear(4096,2048), #Lineal que sale a las clases objetivo
        nn.BatchNorm1d(2048),
        nn.ReLU(inplace=True),
        nn.Linear(2048,1024), #Lineal que sale a las clases objetivo
        nn.ReLU(inplace=True),
        nn.Linear(1024,128), #Lineal que sale a las clases objetivo
        nn.ReLU(inplace=True),
        nn.Dropout(0.2),
        nn.Linear(128,3), #Lineal que sale a las clases objetivo
        nn.Softmax( dim = 1))


  def forward(self, x):
    #print("In:", x.shape)            
    if(self.transformer != None):
      x = self.transformer(x)
    #print("Transformed:", x.shape)
    #print("In:", x.shape)            
    x = x.unsqueeze(1)
    #print("squeeze:", x.shape)
    x = self.inPut(x)
    #print("inPut_res:", x.shape)
    x = self.chanelUp(x)
    #print("chanelUp_res:", x.shape)
    x = self.justTime(x)
    #print("justTime_res:", x.shape)
    #  print(x.shape)         
    x = torch.flatten(x,1)
    #print("post flatten:", x.shape)
    x = self.out(x)
    return  x

##################
####### V7 #######
##################

class onlyWoman_MFCC_16k_v7(_Clasificador_padre):
  """Copia de la arquitectura VGGISH. Agrego capas de batchnorm"""
  def __init__(self, classes, MFCC_transformer = None):
    super().__init__(classes)
    self.transformer = MFCC_transformer
    self.name = "VGGISH"
    ########################
    ## Baloques de la red ##
    ########################
    self.inPut = nn.Sequential(
      nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1,),
      nn.BatchNorm2d(32),
      nn.ReLU(inplace=True),
      nn.Dropout2d(0.2),
      nn.MaxPool2d(kernel_size=2, stride=2)
      )

    self.chanelUp =  nn.Sequential(
      nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1,),
      nn.ReLU(inplace=True),
      nn.MaxPool2d(kernel_size=2, stride=2),

      nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1,),
      nn.ReLU(inplace=True),
      nn.MaxPool2d(kernel_size=2, stride=2),

      nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1,),
      nn.BatchNorm2d(256),
      nn.ReLU(inplace=True),
      nn.Dropout2d(0.2),
      nn.MaxPool2d(kernel_size=2, stride=2)


      )

    self.justTime =  nn.Sequential( #bloque que recibe los datos y los acondiciona
      nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1,),
      nn.ReLU(inplace=True),
      nn.Dropout2d(0.2),
      nn.MaxPool2d(kernel_size=2, stride=2)
        )
 
    self.out = nn.Sequential(   # bloque que acondiciona la salida
        nn.Linear(7680,2048), #Lineal que sale a las clases objetivo
        nn.ReLU(inplace=True),
        #nn.Dropout(0.2),
        #nn.Linear(4096,2048), #Lineal que sale a las clases objetivo
        #nn.BatchNorm1d(2048),
        #nn.ReLU(inplace=True),
        nn.Linear(2048,128), #Lineal que sale a las clases objetivo
        #nn.ReLU(inplace=True),
        #nn.Linear(1024,128), #Lineal que sale a las clases objetivo
        nn.BatchNorm1d(128),
        nn.ReLU(inplace=True),
        nn.Dropout(0.2),
        nn.Linear(128,3), #Lineal que sale a las clases objetivo
        nn.Softmax( dim = 1))


  def forward(self, x):
    #print("In:", x.shape)            
    if(self.transformer != None):
      x = self.transformer(x)
    #print("Transformed:", x.shape)
    #print("In:", x.shape)            
    x = x.unsqueeze(1)
    #print("squeeze:", x.shape)
    x = self.inPut(x)
    #print("inPut_res:", x.shape)
    x = self.chanelUp(x)
    #print("chanelUp_res:", x.shape)
    x = self.justTime(x)
    #print("justTime_res:", x.shape)
    #  print(x.shape)         
    x = torch.flatten(x,1)
    #print("post flatten:", x.shape)
    x = self.out(x)
    return  x



##################
####### V8 #######
##################

class onlyWoman_MFCC_16k_v8(_Clasificador_padre):
  """Copia de la arquitectura VGGISH. Agrego capas de batchnorm"""
  def __init__(self, classes, MFCC_transformer = None):
    super().__init__(classes)
    self.transformer = MFCC_transformer
    self.name = "VGGISH"
    ########################
    ## Baloques de la red ##
    ########################
    self.inPut = nn.Sequential(
      nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1,),
      nn.BatchNorm2d(16),
      nn.ReLU(inplace=True),
      #nn.Dropout2d(0.2),
      nn.MaxPool2d(kernel_size=2, stride=2)
      )

    self.chanelUp =  nn.Sequential(
      nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1,),
      nn.ReLU(inplace=True),
      nn.MaxPool2d(kernel_size=2, stride=2),

      nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1,),
      nn.ReLU(inplace=True),
      nn.MaxPool2d(kernel_size=2, stride=2),
      )

    self.justTime =  nn.Sequential( #bloque que recibe los datos y los acondiciona
      nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1,),
      nn.ReLU(inplace=True),
      nn.MaxPool2d(kernel_size=2, stride=2),

      nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1,),
      nn.ReLU(inplace=True),
      nn.Dropout2d(0.2),
      nn.MaxPool2d(kernel_size=2, stride=2)
        )
 
    self.out = nn.Sequential(   # bloque que acondiciona la salida
        nn.Linear(960,512), #Lineal que sale a las clases objetivo
        nn.ReLU(inplace=True),
        
        nn.Linear(512,128), #Lineal que sale a las clases objetivo
        nn.BatchNorm1d(128),
        nn.ReLU(inplace=True),
        nn.Dropout(0.2),
        
        nn.Linear(128,5), #Lineal que sale a las clases objetivo
        nn.Softmax( dim = 1))


  def forward(self, x):
    #print("In:", x.shape)            
    if(self.transformer != None):
      x = self.transformer(x)
    #print("Transformed:", x.shape)
    #print("In:", x.shape)            
    x = x.unsqueeze(1)
    #print("squeeze:", x.shape)
    x = self.inPut(x)
    #print("inPut_res:", x.shape)
    x = self.chanelUp(x)
    #print("chanelUp_res:", x.shape)
    x = self.justTime(x)
    #print("justTime_res:", x.shape)
    #  print(x.shape)         
    x = torch.flatten(x,1)
    #print("post flatten:", x.shape)
    x = self.out(x)
    return  x